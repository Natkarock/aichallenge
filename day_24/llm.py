from __future__ import annotations

import os
import json
from typing import List, Dict, Any, Optional

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.tools import tool
from langchain.agents import create_agent

# –õ–æ–∫–∞–ª—å–Ω—ã–π RAG-—Ö—Ä–∞–Ω–∏–ª–∏—â–µ (FAISS + –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)
# similarity_search —É–∂–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –≤ rag_store –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ backend –¥–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞.
try:
    from rag_store import similarity_search as _similarity_search
except Exception:
    _similarity_search = (
        None  # —á—Ç–æ–±—ã –º–æ–¥—É–ª—å –º–æ–∂–Ω–æ –±—ã–ª–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–∞–∂–µ –±–µ–∑ rag_store
    )

SYSTEM_PROMPT = "–¢—ã –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫."
WINDOW_MESSAGES = int(os.getenv("CONTEXT_WINDOW_MESSAGES", "12"))
SUMMARY_TARGET_TOKENS = int(os.getenv("SUMMARY_TARGET_TOKENS", "250"))


def _chat_model() -> ChatOpenAI:
    """
    –ï–¥–∏–Ω–∞—è —Ç–æ—á–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è LLM-–º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ LangChain.

    –ú–æ–¥–µ–ª—å –±–µ—Ä—ë—Ç—Å—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è OPENAI_MODEL (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é gpt-4.1-mini).
    """
    model_name = os.getenv("OPENAI_MODEL", "gpt-4.1-mini")
    # ChatOpenAI —Å–∞–º –≤–æ–∑—å–º—ë—Ç OPENAI_API_KEY –∏–∑ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    return ChatOpenAI(model=model_name, temperature=0.3)


def summarize_messages(
    previous_summary: Optional[str], messages: List[Dict[str, str]]
) -> str:
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ summary –¥–∏–∞–ª–æ–≥–∞.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç LangChain ChatOpenAI –≤–º–µ—Å—Ç–æ –ø—Ä—è–º–æ–≥–æ –≤—ã–∑–æ–≤–∞ openai.responses.
    """
    instruction = (
        "–°—É–º–º–∏—Ä—É–π –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ –≤ –∫—Ä–∞—Ç–∫–∏–π –∫–æ–Ω—Å–ø–µ–∫—Ç –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. "
        "–°–æ—Ö—Ä–∞–Ω—è–π —Ñ–∞–∫—Ç—ã, –≤–≤–æ–¥—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –ø—Ä–∏–Ω—è—Ç—ã–µ —Ä–µ—à–µ–Ω–∏—è –∏ –¥–æ–≥–æ–≤–æ—Ä—ë–Ω–Ω–æ—Å—Ç–∏. "
        f"–û–±–Ω–æ–≤–∏ –ø—Ä–µ–∂–Ω—é—é —Å–≤–æ–¥–∫—É (–µ—Å–ª–∏ –¥–∞–Ω–∞). –û–±—ä–µ–º ~{SUMMARY_TARGET_TOKENS} —Ç–æ–∫–µ–Ω–æ–≤."
    )
    if not messages:
        return previous_summary or ""

    try:
        last_n = WINDOW_MESSAGES
        ctx = messages[-last_n:]
        transcript_lines: List[str] = []
        for m in ctx:
            role = m.get("role")
            content = m.get("content", "")
            transcript_lines.append(f"{role}: {content}")
        transcript = "\n".join(transcript_lines)
        prev = previous_summary or "(–ø—É—Å—Ç–æ)"

        llm = _chat_model()
        lc_messages = [
            SystemMessage(content="–¢—ã –ª–∞–∫–æ–Ω–∏—á–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏."),
            HumanMessage(
                content=(
                    f"{instruction}\n\n"
                    f"–ü—Ä–µ–¥—ã–¥—É—â–∞—è —Å–≤–æ–¥–∫–∞:\n{prev}\n\n"
                    f"–ù–æ–≤–∞—è —Å—Ç–µ–Ω–æ–≥—Ä–∞–º–º–∞:\n{transcript}\n\n"
                    "–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—É—é —Å–≤–æ–¥–∫—É."
                )
            ),
        ]
        resp = llm.invoke(lc_messages)
        return (resp.content or "").strip()
    except Exception:
        # –í —Å–ª—É—á–∞–µ –ª—é–±—ã—Ö –ø—Ä–æ–±–ª–µ–º ‚Äî –ø—Ä–æ—Å—Ç–æ –≤–µ—Ä–Ω—ë–º —Å—Ç–∞—Ä–æ–µ summary,
        # —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å –æ—Å–Ω–æ–≤–Ω—É—é –ª–æ–≥–∏–∫—É –¥–∏–∞–ª–æ–≥–∞.
        return previous_summary or ""


def generate_reply(messages: List[Dict[str, str]], summary: Optional[str]) -> str:
    """
    –ë–∞–∑–æ–≤—ã–π –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –ë–ï–ó RAG.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç LangChain ChatOpenAI.
    """
    if not messages:
        return "–ü—Ä–∏–≤–µ—Ç! –û —á—ë–º –ø–æ–≥–æ–≤–æ—Ä–∏–º?"

    ctx = messages[-WINDOW_MESSAGES:]

    try:
        llm = _chat_model()
        lc_messages = [SystemMessage(content=SYSTEM_PROMPT)]

        if summary:
            lc_messages.append(
                SystemMessage(content=f"–ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {summary}")
            )

        for m in ctx:
            role = m.get("role", "user")
            content = m.get("content", "")
            if role == "assistant":
                lc_messages.append(AIMessage(content=content))
            else:
                # –≤—Å—ë –æ—Å—Ç–∞–ª—å–Ω–æ–µ —Å—á–∏—Ç–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º –≤–≤–æ–¥–æ–º
                lc_messages.append(HumanMessage(content=content))

        resp = llm.invoke(lc_messages)
        return (resp.content or "").strip()
    except Exception as e:
        # –ü–æ–≤–µ–¥–µ–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ —Å—Ç–∞—Ä—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é:
        # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –ø–∞–¥–µ–Ω–∏—è.
        last_user = next(
            (m.get("content", "") for m in reversed(ctx) if m.get("role") == "user"),
            "",
        )
        if last_user:
            return (
                "–Ø –Ω–µ —Å–º–æ–≥ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç. "
                f"–ü–æ–≤—Ç–æ—Ä—é —Ç–≤–æ–π –≤–æ–ø—Ä–æ—Å: {last_user}"
            )
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ LLM: {e}"


def _format_context(docs):
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–ø–∏—Å–æ–∫ Document (–∏–ª–∏ dict-–ø–æ–¥–æ–±–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤) –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –±–ª–æ–∫
    –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –≤ prompt.
    """
    lines: List[str] = []
    for d in docs or []:
        try:
            meta = (
                getattr(d, "metadata", {})
                if hasattr(d, "metadata")
                else d.get("metadata", {})
            )
            content = (
                getattr(d, "page_content", "")
                if hasattr(d, "page_content")
                else d.get("page_content", "")
            )
        except Exception:
            meta, content = {}, ""
        lines.append(f"Source: {meta}\nContent: {content}")
    return "\n\n".join(lines)


def generate_rag_reply(
    question: str,
    summary: Optional[str],
    retrieved_docs,
    with_sources: bool = False,
) -> str:
    """
    –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAG.

    –ï—Å–ª–∏ with_sources=True, –≤ –ø—Ä–æ–º–ø—Ç –ø–µ—Ä–µ–¥–∞—ë—Ç—Å—è —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (file_path, file_name, snippet),
    –∏ LLM –ø–æ–ª—É—á–∞–µ—Ç –∂—ë—Å—Ç–∫—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –≤ –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞ –¥–æ–±–∞–≤–∏—Ç—å —Ä–∞–∑–¥–µ–ª "–ò—Å—Ç–æ—á–Ω–∏–∫–∏:"
    —Å –ø—É—Ç—è–º–∏ –∫ —Ñ–∞–π–ª–∞–º.

    –í–ê–ñ–ù–û: retrieved_docs —Å—é–¥–∞ –ø–æ-–ø—Ä–µ–∂–Ω–µ–º—É –ø—Ä–æ–±—Ä–∞—Å—ã–≤–∞—é—Ç—Å—è –∏–∑ main.py,
    —á—Ç–æ–±—ã UI –º–æ–≥ –æ—Ç—Ä–∏—Å–æ–≤–∞—Ç—å –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏. –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
    —É–∂–µ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –∏ –∫–∞–∫ LangChain tool (—Å–º. retrieve_context –Ω–∏–∂–µ).
    """
    context_block = _format_context(retrieved_docs)

    # –ü—Ä–æ—Å—Ç–æ–π —Ä–µ–∂–∏–º: –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é, –±–µ–∑ –∂—ë—Å—Ç–∫–æ–≥–æ –±–ª–æ–∫–∞ ¬´–ò—Å—Ç–æ—á–Ω–∏–∫–∏¬ª
    if not with_sources:
        combined = f"–í–æ–ø—Ä–æ—Å: {question}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç (–∏–∑ RAG):\n{context_block}"
        return generate_reply(
            [{"role": "user", "content": combined}],
            summary=summary,
        )

    # ---- 1. –°–æ–±–∏—Ä–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ –¥–µ–¥—É–ø–ª–∏–º –ø–æ file_path ----
    sources_by_path: Dict[str, Dict[str, Any]] = {}

    for d in retrieved_docs or []:
        try:
            meta = (
                getattr(d, "metadata", {})
                if hasattr(d, "metadata")
                else d.get("metadata", {})
            )
            content = (
                getattr(d, "page_content", "")
                if hasattr(d, "page_content")
                else d.get("page_content", "")
            )
        except Exception:
            meta, content = {}, ""

        file_path = meta.get("file_path") or meta.get("source") or "unknown"
        file_name = meta.get("file_name") or meta.get("source") or file_path
        snippet = (content or "").strip()[:200]

        if file_path not in sources_by_path:
            sources_by_path[file_path] = {
                "file_path": file_path,
                "file_name": file_name,
                "snippets": [],
            }
        if snippet and snippet not in sources_by_path[file_path]["snippets"]:
            sources_by_path[file_path]["snippets"].append(snippet)

    sources: List[Dict[str, Any]] = []
    for _fp, data in sources_by_path.items():
        snippets = data.get("snippets") or []
        full_snippet = " ".join(snippets)
        if len(full_snippet) > 200:
            full_snippet = full_snippet[:197] + "..."
        sources.append(
            {
                "file_path": data["file_path"],
                "file_name": data["file_name"],
                "snippet": full_snippet,
            }
        )

    try:
        sources_json = json.dumps(sources, ensure_ascii=False, indent=2)
    except Exception:
        sources_json = "[]"

    prompt = f"""
–¢—ã –æ—Ç–≤–µ—á–∞–µ—à—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ —Å–ø–∏—Å–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.

–¢–µ–±–µ –¥–∞–Ω—ã:
1) –í–æ–ø—Ä–æ—Å.
2) –ö–æ–Ω—Ç–µ–∫—Å—Ç —Å –≤—ã–¥–µ—Ä–∂–∫–∞–º–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
3) –°–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON.

–¢–í–û–Ø –ó–ê–î–ê–ß–ê:
- –î–∞–π —Å–≤—è–∑–Ω—ã–π, –ø–æ–Ω—è—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
- –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –ø—Ä–∏–≤–µ–¥—ë–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç. –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π —Ñ–∞–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç.
- –í –ö–û–ù–¶–ï –æ—Ç–≤–µ—Ç–∞ –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –¥–æ–±–∞–≤—å —Ä–∞–∑–¥–µ–ª "–ò—Å—Ç–æ—á–Ω–∏–∫–∏:".
- –í —Ä–∞–∑–¥–µ–ª–µ "–ò—Å—Ç–æ—á–Ω–∏–∫–∏:" –≤—ã–≤–µ–¥–∏ –ø–æ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ –Ω–∞ –∫–∞–∂–¥—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–∑ JSON.
- –§–æ—Ä–º–∞—Ç –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ –≤ —Ä–∞–∑–¥–µ–ª–µ "–ò—Å—Ç–æ—á–Ω–∏–∫–∏:":
 file_name - –∫—Ä–∞c–æ—á–Ω–æ–µ –ø–æ—è—Å–Ω–µ–Ω–∏–µ
- –ù–ï –≤—ã–≤–æ–¥–∏ –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ file_path –∏–ª–∏ file_name –±–æ–ª—å—à–µ 1 —Ä–∞–∑–∞.

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
1. –°–Ω–∞—á–∞–ª–∞ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç.
2. –ü–æ—Ç–æ–º –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞.
3. –ó–∞—Ç–µ–º —Å—Ç—Ä–æ–∫–∞ "–ò—Å—Ç–æ—á–Ω–∏–∫–∏:" –∏ —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É.

–í–æ–ø—Ä–æ—Å:
{question}

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context_block}

–°–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (JSON):
{sources_json}
""".strip()

    return generate_reply(
        [{"role": "user", "content": prompt}],
        summary=summary,
    )


def judge_rag_help(question: str, baseline: str, rag_answer: str) -> str:
    """
    –ù–µ–±–æ–ª—å—à–∞—è —É—Ç–∏–ª–∏—Ç–∞, –∫–æ—Ç–æ—Ä–∞—è —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –æ—Ç–≤–µ—Ç—ã ¬´—Å RAG¬ª –∏ ¬´–±–µ–∑ RAG¬ª.
    """
    prompt = (
        "–û–ø—Ä–µ–¥–µ–ª–∏, –ø–æ–º–æ–≥ –ª–∏ RAG —É–ª—É—á—à–∏—Ç—å –æ—Ç–≤–µ—Ç. "
        "–ö—Ä–∏—Ç–µ—Ä–∏–∏: —Ç–æ—á–Ω–æ—Å—Ç—å —Ñ–∞–∫—Ç–æ–≤, –ø–æ–ª–Ω–æ—Ç–∞, –Ω–∞–ª–∏—á–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç. "
        "–í–µ—Ä–Ω–∏ –∫–æ—Ä–æ—Ç–∫–∏–π –≤—ã–≤–æ–¥ –≤ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö.\n\n"
        f"–í–æ–ø—Ä–æ—Å: {question}\n---\n–ë–µ–∑ RAG:\n{baseline}\n---\n–° RAG:\n{rag_answer}"
    )
    return generate_reply([{"role": "user", "content": prompt}], summary=None)


# =============================================================================
# üîß LangChain tool –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ similarity_search
# =============================================================================


@tool(response_format="content_and_artifact")
def retrieve_context(query: str, k: int = 3):
    """
    Retrieve information to help answer a query –∏–∑ RAG-—Ö—Ä–∞–Ω–∏–ª–∏—â–∞.

    –ü–æ–¥ –∫–∞–ø–æ—Ç–æ–º –≤—ã–∑—ã–≤–∞–µ—Ç rag_store.similarity_search, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å top-k –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      - content: —Å–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏ —Å–æ–¥–µ—Ä–∂–∏–º—ã–º,
      - artifact: –∏—Å—Ö–æ–¥–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞–≥–µ–Ω—Ç–æ–º).
    """
    if _similarity_search is None:
        return "RAG-—Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ (similarity_search –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ).", []

    retrieved_docs = _similarity_search(query, k=k)
    serialized = "\n\n".join(
        "Source: {meta}\nContent: {content}".format(
            meta=getattr(doc, "metadata", {}),
            content=getattr(doc, "page_content", ""),
        )
        for doc in retrieved_docs
    )
    return serialized, retrieved_docs


TOOLS = [retrieve_context]


def create_rag_agent(
    model: Optional[str] = None,
):
    """
    –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ–∞–±—Ä–∏–∫–∞ –¥–ª—è LangGraph-–∞–≥–µ–Ω—Ç–∞ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º retrieve_context.

    –ö–∞–∫ –≤ weather_mcp: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º runnable-–≥—Ä–∞—Ñ, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–Ω–∏–º–∞–µ—Ç {"messages": [...]}.
    """
    model_name = model or os.getenv("OPENAI_MODEL", "gpt-4.1-mini")
    lc_model_id = f"openai:{model_name}"

    # –ë–µ–∑ system_prompt ‚Äî –µ–≥–æ –±—É–¥–µ–º –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —á–µ—Ä–µ–∑ SystemMessage –ø—Ä–∏ –≤—ã–∑–æ–≤–µ
    agent = create_agent(
        model=lc_model_id,
        tools=TOOLS,
    )
    return agent


def generate_agent_rag_reply(
    question: str,
    summary: Optional[str] = None,
    model: Optional[str] = None,
) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ LangGraph-–∞–≥–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π —Å–∞–º –≤—ã–∑—ã–≤–∞–µ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç retrieve_context.

    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
        result = generate_agent_rag_reply("–ö–∞–∫ –≥–æ—Ç–æ–≤–∏—Ç—å –ø–ª–æ–≤?")
    """
    try:
        agent = create_rag_agent(model=model)

        base_system_prompt = (
            "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –∏—Å–ø–æ–ª—å–∑—É—è RAG-—Ö—Ä–∞–Ω–∏–ª–∏—â–µ.\n"
            "–£ —Ç–µ–±—è –µ—Å—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç retrieve_context, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\n"
            "–ö–æ–≥–¥–∞ –≤–æ–ø—Ä–æ—Å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤—ã–∑—ã–≤–∞–π —ç—Ç–æ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç. "
            "–ü–æ–¥–∞–≤–∞–π –Ω–∞ –≤—Ö–æ–¥ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ë–ï–ó –∏—Å–∫–∞–∂–µ–Ω–∏–π. "
            "–ó–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–π –ø–æ–ª—É—á–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞."
        )

        if summary:
            base_system_prompt += (
                "\n\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞ –¥–∏–∞–ª–æ–≥–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:\n" + summary
            )

        messages = [
            SystemMessage(content=base_system_prompt),
            HumanMessage(content=question),
        ]

        result = agent.invoke({"messages": messages})

        # –î–ª—è create_agent/CompiledStateGraph —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—ã—á–Ω–æ:
        # {"messages": [SystemMessage, ..., AIMessage]}
        if isinstance(result, dict):
            if "messages" in result:
                msgs = result["messages"]
                if isinstance(msgs, list) and msgs:
                    # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π AIMessage
                    for m in reversed(msgs):
                        if isinstance(m, AIMessage):
                            return (m.content or "").strip()
                    # –ï—Å–ª–∏ AIMessage –Ω–µ –Ω–∞—à–ª–∏ ‚Äî –±–µ—Ä—ë–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ
                    last = msgs[-1]
                    return getattr(last, "content", str(last))
            # –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π ‚Äî –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞ —Å "output"
            if "output" in result:
                return str(result["output"])

        # –§–æ–ª–ª–±–µ–∫ ‚Äî –ø—Ä–æ—Å—Ç–æ —Å—Ç—Ä–æ–∫–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
        return str(result)

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –∞–≥–µ–Ω—Ç–∞: {e}"
