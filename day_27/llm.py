from __future__ import annotations

import os
import json
from typing import List, Dict, Any, Optional

from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.tools import tool
from langchain.agents import create_agent

# –õ–æ–∫–∞–ª—å–Ω—ã–π RAG-—Ö—Ä–∞–Ω–∏–ª–∏—â–µ (FAISS + –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)
# similarity_search —É–∂–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –≤ rag_store –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ backend –¥–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞.
try:
    from rag_store import similarity_search as _similarity_search
except Exception:
    _similarity_search = (
        None  # —á—Ç–æ–±—ã –º–æ–¥—É–ª—å –º–æ–∂–Ω–æ –±—ã–ª–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–∞–∂–µ –±–µ–∑ rag_store
    )

# ---------------------------------------------------------
# üîß –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è: –∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
# ---------------------------------------------------------

USER_CONFIG_PATH = os.getenv("USER_CONFIG_PATH", "user_config.json")
IS_LOCAL = True


def get_is_local() -> bool:
    return IS_LOCAL


def set_is_Local(isLocal: bool):
    global IS_LOCAL
    IS_LOCAL = isLocal


def _load_user_config() -> Dict[str, Any]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç JSON-–∫–æ–Ω—Ñ–∏–≥ —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
    –ï—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç –∏–ª–∏ –æ–Ω –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç–æ–π dict.
    """
    try:
        with open(USER_CONFIG_PATH, "r", encoding="utf-8") as f:
            data = json.load(f)
        if isinstance(data, dict):
            return data
    except FileNotFoundError:
        # –ü—Ä–æ—Å—Ç–æ —Ä–∞–±–æ—Ç–∞–µ–º –±–µ–∑ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏
        pass
    except Exception:
        # –ù–µ –ø–∞–¥–∞–µ–º, —á—Ç–æ–±—ã –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–µ –ª–æ–º–∞–ª–æ—Å—å –∏–∑-–∑–∞ –∫–æ–Ω—Ñ–∏–≥–∞
        pass
    return {}


USER_CONFIG: Dict[str, Any] = _load_user_config()


def _build_system_prompt() -> str:
    """
    –°—Ç—Ä–æ–∏—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π system prompt –Ω–∞ –æ—Å–Ω–æ–≤–µ USER_CONFIG.

    –ö–æ–Ω—Ñ–∏–≥ –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–ö–∞—Ä–∞–ø–∞—Ü –ù–∞—Ç–∞–ª–∏—é),
    –∞ —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç –¥–∞—ë—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—É –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, –∫–∞–∫ —Å –Ω–µ–π —Ä–∞–±–æ—Ç–∞—Ç—å.
    """
    base = "–¢—ã –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –Ω–∞–ø–∞—Ä–Ω–∏–∫."

    profile = USER_CONFIG or {}
    name = profile.get("name", "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")
    age = profile.get("age")
    role = profile.get("role")
    hobbies = profile.get("hobbies") or []
    preferences = profile.get("preferences") or {}
    dev_focus = preferences.get("dev_focus")
    comm_style = preferences.get("communication_style")
    likes = preferences.get("likes")
    dislikes = preferences.get("dislikes")

    parts: List[str] = [base, ""]

    # –û—Å–Ω–æ–≤–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    main_line = f"–¢–≤–æ–π –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å ‚Äî {name}. "
    if age is not None:
        main_line += f"–ï–π {age} –≥–æ–¥–∞. "
    if role:
        main_line += f"–û–Ω–∞ {role}. "
    parts.append(main_line.strip())

    # –•–æ–±–±–∏
    if hobbies:
        parts.append("–í —Å–≤–æ–±–æ–¥–Ω–æ–µ –≤—Ä–µ–º—è –æ–Ω–∞ —É–≤–ª–µ–∫–∞–µ—Ç—Å—è: " + ", ".join(hobbies) + ".")

    # –§–æ–∫—É—Å –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ / —á–µ–º –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è
    if dev_focus:
        parts.append(dev_focus)

    # –ü—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π —Å—Ç–∏–ª—å –æ–±—â–µ–Ω–∏—è
    if comm_style:
        parts.append(comm_style)

    # –ß—Ç–æ –µ–π –æ—Å–æ–±–µ–Ω–Ω–æ –Ω—Ä–∞–≤–∏—Ç—Å—è / –Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è –≤ –æ—Ç–≤–µ—Ç–∞—Ö
    if likes:
        parts.append("–û–Ω–∞ —ç—Ç–æ —Ü–µ–Ω–∏—Ç: " + likes)
    if dislikes:
        parts.append("–≠—Ç–æ–≥–æ —Å—Ç–æ–∏—Ç –∏–∑–±–µ–≥–∞—Ç—å: " + dislikes)

    # –û–±—â–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –æ—Ç–≤–µ—Ç–∞–º
    parts.append(
        "–í—Å–µ–≥–¥–∞ –ø–∏—à–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –µ—Å–ª–∏ —è–≤–Ω–æ –Ω–µ –ø–æ–ø—Ä–æ—Å–∏–ª–∏ –∏–Ω–∞—á–µ. "
        "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –æ—Ç–≤–µ—Ç—ã: —Å–Ω–∞—á–∞–ª–∞ –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ, "
        "–∑–∞—Ç–µ–º –¥–µ—Ç–∞–ª–∏ —Å –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –∏ —Å–ø–∏—Å–∫–∞–º–∏. "
        "–ß–∞—â–µ –¥–∞–≤–∞–π –≥–æ—Ç–æ–≤—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∫–æ–¥–∞, —Ü–µ–ª—ã–µ —Ñ–∞–π–ª—ã –∏ –ø–æ—à–∞–≥–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. "
        "–ò—Å–ø–æ–ª—å–∑—É–π –∑–Ω–∞–Ω–∏—è –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ, —á—Ç–æ–±—ã –¥–µ–ª–∞—Ç—å –æ—Ç–≤–µ—Ç—ã –±–æ–ª–µ–µ —É–º–µ—Å—Ç–Ω—ã–º–∏ "
        "–∏–º–µ–Ω–Ω–æ –¥–ª—è –Ω–µ—ë, –Ω–æ –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π —Ñ–∞–∫—Ç—ã –æ –µ—ë –ª–∏—á–Ω–æ–π –∂–∏–∑–Ω–∏ –∏ –Ω–µ —Ä–∞—Å–∫—Ä—ã–≤–∞–π "
        "—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∫–æ–Ω—Ñ–∏–≥–∞, –µ—Å–ª–∏ —Ç–µ–±—è –ø—Ä—è–º–æ –æ–± —ç—Ç–æ–º –Ω–µ –ø–æ–ø—Ä–æ—Å–∏–ª–∏."
    )

    return "\n".join(p for p in parts if p)


SYSTEM_PROMPT = _build_system_prompt()
# "–¢—ã –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç"
WINDOW_MESSAGES = int(os.getenv("CONTEXT_WINDOW_MESSAGES", "12"))
SUMMARY_TARGET_TOKENS = int(os.getenv("SUMMARY_TARGET_TOKENS", "250"))


def _chat_model() -> ChatOpenAI:
    """
    –ï–¥–∏–Ω–∞—è —Ç–æ—á–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è LLM-–º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ LangChain.

    –ú–æ–¥–µ–ª—å –±–µ—Ä—ë—Ç—Å—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è OPENAI_MODEL (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é gpt-4.1-mini).
    """
    if IS_LOCAL:
        return ChatOllama(model="qwen2.5:1.5b", base_url="http://localhost:11434")
    else:
        model_name = os.getenv("OPENAI_MODEL", "gpt-4.1-mini")
        temperature = float(os.getenv("OPENAI_TEMPERATURE", "0.3"))
        timeout = float(os.getenv("OPENAI_TIMEOUT", "60"))
        max_tokens = int(os.getenv("OPENAI_MAX_TOKENS", "1024"))
        return ChatOpenAI(
            model=model_name,
            temperature=temperature,
            timeout=timeout,
            max_tokens=max_tokens,
        )


def summarize_messages(
    previous_summary: Optional[str], messages: List[Dict[str, str]]
) -> str:
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ summary –¥–∏–∞–ª–æ–≥–∞.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç LangChain ChatOpenAI.
    """
    instruction = (
        "–°—É–º–º–∏—Ä—É–π –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ –≤ –∫—Ä–∞—Ç–∫–∏–π –∫–æ–Ω—Å–ø–µ–∫—Ç –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. "
        "–°–æ—Ö—Ä–∞–Ω—è–π —Ñ–∞–∫—Ç—ã, –≤–≤–æ–¥—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –ø—Ä–∏–Ω—è—Ç—ã–µ —Ä–µ—à–µ–Ω–∏—è –∏ –¥–æ–≥–æ–≤–æ—Ä—ë–Ω–Ω–æ—Å—Ç–∏. "
        f"–û–±–Ω–æ–≤–∏ –ø—Ä–µ–∂–Ω—é—é —Å–≤–æ–¥–∫—É (–µ—Å–ª–∏ –¥–∞–Ω–∞). –û–±—ä–µ–º ~{SUMMARY_TARGET_TOKENS} —Ç–æ–∫–µ–Ω–æ–≤."
    )

    if not messages:
        return previous_summary or ""

    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ—Å—Ç—É—é —Ç–µ–∫—Å—Ç–æ–≤—É—é —Å—Ç–µ–Ω–æ–≥—Ä–∞–º–º—É
    lines: List[str] = []
    for msg in messages[-WINDOW_MESSAGES:]:
        role = msg.get("role", "user")
        content = msg.get("content", "")
        if role == "assistant":
            prefix = "–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç"
        elif role == "user":
            prefix = "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å"
        else:
            prefix = role
        lines.append(f"{prefix}: {content}")
    transcript = "\n".join(lines)

    if previous_summary:
        user_prompt = (
            f"{instruction}\n\n"
            f"–ü—Ä–µ–¥—ã–¥—É—â–∞—è —Å–≤–æ–¥–∫–∞:\n{previous_summary}\n\n"
            f"–ù–æ–≤–∞—è —Å—Ç–µ–Ω–æ–≥—Ä–∞–º–º–∞:\n{transcript}\n\n"
            "–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—É—é —Å–≤–æ–¥–∫—É."
        )
    else:
        user_prompt = (
            f"{instruction}\n\n"
            f"–°—Ç–µ–Ω–æ–≥—Ä–∞–º–º–∞:\n{transcript}\n\n"
            "–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ —Å–≤–æ–¥–∫—É –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤."
        )

    try:
        llm = _chat_model()
        lc_messages = [
            SystemMessage(
                content=(
                    "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π —Å–∂–∏–º–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ –≤ –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É. "
                    "–ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–æ–≤—ã—Ö —Ñ–∞–∫—Ç–æ–≤ –∏ –Ω–µ –¥–∞–≤–∞–π —Å–æ–≤–µ—Ç–æ–≤."
                )
            ),
            HumanMessage(content=user_prompt),
        ]
        resp = llm.invoke(lc_messages)
        return (resp.content or "").strip()
    except Exception:
        # –í —Å–ª—É—á–∞–µ –ª—é–±—ã—Ö –ø—Ä–æ–±–ª–µ–º ‚Äî –ø—Ä–æ—Å—Ç–æ –≤–µ—Ä–Ω—ë–º —Å—Ç–∞—Ä–æ–µ summary,
        # —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å –æ—Å–Ω–æ–≤–Ω—É—é –ª–æ–≥–∏–∫—É –¥–∏–∞–ª–æ–≥–∞.
        return previous_summary or ""


def generate_reply(messages: List[Dict[str, str]], summary: Optional[str]) -> str:
    """
    –ë–∞–∑–æ–≤—ã–π –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –ë–ï–ó RAG.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç LangChain ChatOpenAI.
    """
    if not messages:
        return "–ü—Ä–∏–≤–µ—Ç! –û —á—ë–º –ø–æ–≥–æ–≤–æ—Ä–∏–º?"

    ctx = messages[-WINDOW_MESSAGES:]

    try:
        llm = _chat_model()
        lc_messages: List[Any] = [SystemMessage(content=SYSTEM_PROMPT)]

        if summary:
            lc_messages.append(
                SystemMessage(content=f"–ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {summary}")
            )

        for m in ctx:
            role = m.get("role", "user")
            content = m.get("content", "")
            if role == "assistant":
                lc_messages.append(AIMessage(content=content))
            else:
                # –≤—Å—ë –æ—Å—Ç–∞–ª—å–Ω–æ–µ —Å—á–∏—Ç–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º –≤–≤–æ–¥–æ–º
                lc_messages.append(HumanMessage(content=content))

        resp = llm.invoke(lc_messages)
        return (resp.content or "").strip()
    except Exception as e:
        # –ü–æ–≤–µ–¥–µ–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ —Å—Ç–∞—Ä—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é:
        # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –ø–∞–¥–µ–Ω–∏—è.
        last_user = next(
            (
                m.get("content", "")
                for m in reversed(messages)
                if m.get("role") == "user"
            ),
            "",
        )
        return f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç. –û—à–∏–±–∫–∞: {e}. –ü–æ—Å–ª–µ–¥–Ω–∏–π –∑–∞–ø—Ä–æ—Å: {last_user!r}"


def _format_context(docs):
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–ø–∏—Å–æ–∫ Document (–∏–ª–∏ dict-–ø–æ–¥–æ–±–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤) –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –±–ª–æ–∫
    –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –≤ prompt.
    """
    lines: List[str] = []
    for d in docs or []:
        try:
            meta = (
                getattr(d, "metadata", {})
                if hasattr(d, "metadata")
                else d.get("metadata", {})
            )
            content = (
                getattr(d, "page_content", "")
                if hasattr(d, "page_content")
                else d.get("page_content", "")
            )
        except Exception:
            meta, content = {}, ""
        lines.append(f"Source: {meta}\nContent: {content}")
    return "\n\n".join(lines)


def generate_rag_reply(
    question: str,
    summary: Optional[str],
    retrieved_docs,
    with_sources: bool = False,
) -> str:
    """
    –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAG.

    –ï—Å–ª–∏ with_sources=True, –≤ –ø—Ä–æ–º–ø—Ç –ø–µ—Ä–µ–¥–∞—ë—Ç—Å—è —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (file_path, file_name, snippet),
    –∏ LLM –ø–æ–ª—É—á–∞–µ—Ç –∂—ë—Å—Ç–∫—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –≤ –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞ –¥–æ–±–∞–≤–∏—Ç—å —Ä–∞–∑–¥–µ–ª "–ò—Å—Ç–æ—á–Ω–∏–∫–∏:"
    —Å –ø—É—Ç—è–º–∏ –∫ —Ñ–∞–π–ª–∞–º.
    """
    context_block = _format_context(retrieved_docs)

    # –ü—Ä–æ—Å—Ç–æ–π —Ä–µ–∂–∏–º: –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é, –±–µ–∑ –∂—ë—Å—Ç–∫–æ–≥–æ –±–ª–æ–∫–∞ ¬´–ò—Å—Ç–æ—á–Ω–∏–∫–∏¬ª
    if not with_sources:
        combined = f"–í–æ–ø—Ä–æ—Å: {question}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç (–∏–∑ RAG):\n{context_block}"
        return generate_reply(
            [
                {"role": "user", "content": combined},
            ],
            summary=summary,
        )

    # –†–∞—Å–∫–ª–∞–¥—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ file_path –∏ —Å–æ–±–∏—Ä–∞–µ–º —É–∫–æ—Ä–æ—á–µ–Ω–Ω—ã–µ —Å–Ω–∏–ø–ø–µ—Ç—ã
    sources_by_path: Dict[str, Dict[str, Any]] = {}
    for d in retrieved_docs or []:
        try:
            meta = (
                getattr(d, "metadata", {})
                if hasattr(d, "metadata")
                else d.get("metadata", {})
            )
            content = (
                getattr(d, "page_content", "")
                if hasattr(d, "page_content")
                else d.get("page_content", "")
            )
        except Exception:
            meta, content = {}, ""

        file_path = meta.get("file_path") or meta.get("source") or "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—É—Ç—å"
        file_name = meta.get("file_name") or file_path
        snippet = (content or "")[:200]

        bucket = sources_by_path.setdefault(
            file_path,
            {"file_path": file_path, "file_name": file_name, "snippets": []},
        )
        if snippet:
            bucket["snippets"].append(snippet)

    sources: List[Dict[str, Any]] = []
    for _fp, data_ in sources_by_path.items():
        snippets = data_.get("snippets") or []
        full_snippet = " ".join(snippets)
        if len(full_snippet) > 200:
            full_snippet = full_snippet[:197] + "..."
        sources.append(
            {
                "file_path": data_["file_path"],
                "file_name": data_["file_name"],
                "snippet": full_snippet,
            }
        )

    try:
        sources_json = json.dumps(sources, ensure_ascii=False, indent=2)
    except Exception:
        sources_json = "[]"

    prompt = f"""
–¢—ã –æ—Ç–≤–µ—á–∞–µ—à—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ –ª–æ–∫–∞–ª—å–Ω—ã–º —Ñ–∞–π–ª–∞–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–°–Ω–∞—á–∞–ª–∞ –¥–∞–π —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–π, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –ø–æ –≤–æ–ø—Ä–æ—Å—É, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ CONTEXT.
–ó–∞—Ç–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–±–∞–≤—å —Ä–∞–∑–¥–µ–ª "–ò—Å—Ç–æ—á–Ω–∏–∫–∏:", –≥–¥–µ –≤ –≤–∏–¥–µ –º–∞—Ä–∫–µ—Ä–æ–≤ –≤—ã–≤–µ–¥–∏ file_path
—Ñ–∞–π–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –ø–æ–ª–µ–∑–Ω—ã –¥–ª—è –æ—Ç–≤–µ—Ç–∞.

CONTEXT (–∏–∑ RAG):
{context_block}

SOURCES (JSON):
{sources_json}

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}
"""
    return generate_reply(
        [
            {"role": "user", "content": prompt},
        ],
        summary=summary,
    )


def judge_rag_help(question: str, baseline: str, rag_answer: str) -> str:
    """
    –ù–µ–±–æ–ª—å—à–∞—è —É—Ç–∏–ª–∏—Ç–∞, –∫–æ—Ç–æ—Ä–∞—è —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –æ—Ç–≤–µ—Ç—ã ¬´—Å RAG¬ª –∏ ¬´–±–µ–∑ RAG¬ª.
    """
    prompt = (
        "–û–ø—Ä–µ–¥–µ–ª–∏, –ø–æ–º–æ–≥ –ª–∏ RAG —É–ª—É—á—à–∏—Ç—å –æ—Ç–≤–µ—Ç. "
        "–ö—Ä–∏—Ç–µ—Ä–∏–∏: —Ç–æ—á–Ω–æ—Å—Ç—å —Ñ–∞–∫—Ç–æ–≤, –ø–æ–ª–Ω–æ—Ç–∞, –Ω–∞–ª–∏—á–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç. "
        "–í–µ—Ä–Ω–∏ –∫–æ—Ä–æ—Ç–∫–∏–π –≤—ã–≤–æ–¥ –≤ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö.\n\n"
        f"–í–æ–ø—Ä–æ—Å: {question}\n---\n–ë–µ–∑ RAG:\n{baseline}\n---\n–° RAG:\n{rag_answer}"
    )
    return generate_reply([{"role": "user", "content": prompt}], summary=None)


# =============================================================================
# üîß LangChain tool –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ similarity_search
# =============================================================================


@tool(response_format="content_and_artifact")
def retrieve_context(query: str, k: int = 3):
    """
    Retrieve information to help answer a query –∏–∑ RAG-—Ö—Ä–∞–Ω–∏–ª–∏—â–∞.

    –ü–æ–¥ –∫–∞–ø–æ—Ç–æ–º –≤—ã–∑—ã–≤–∞–µ—Ç rag_store.similarity_search, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å top-k –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      - content: —Å–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏ —Å–æ–¥–µ—Ä–∂–∏–º—ã–º,
      - artifact: –∏—Å—Ö–æ–¥–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞–≥–µ–Ω—Ç–æ–º).
    """
    if _similarity_search is None:
        return "RAG-—Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ (similarity_search –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ).", []

    retrieved_docs = _similarity_search(query, k=k)
    serialized = "\n\n".join(
        "Source: {meta}\nContent: {content}".format(
            meta=getattr(doc, "metadata", {}),
            content=getattr(doc, "page_content", ""),
        )
        for doc in retrieved_docs
    )
    return serialized, retrieved_docs


TOOLS = [retrieve_context]


def create_rag_agent(
    model: Optional[str] = None,
):
    """
    –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ–∞–±—Ä–∏–∫–∞ –¥–ª—è LangGraph-–∞–≥–µ–Ω—Ç–∞ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º retrieve_context.

    –ö–∞–∫ –≤ weather_mcp: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º runnable-–≥—Ä–∞—Ñ, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–Ω–∏–º–∞–µ—Ç {"messages": [...]}.
    """
    model_name = model or os.getenv("OPENAI_MODEL", "gpt-4.1-mini")
    lc_model_id = f"openai:{model_name}"

    # –ë–µ–∑ system_prompt ‚Äî –µ–≥–æ –±—É–¥–µ–º –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —á–µ—Ä–µ–∑ SystemMessage –ø—Ä–∏ –≤—ã–∑–æ–≤–µ
    agent = create_agent(
        model=lc_model_id,
        tools=TOOLS,
    )
    return agent


def generate_agent_rag_reply(
    question: str,
    summary: Optional[str] = None,
    model: Optional[str] = None,
) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ LangGraph-–∞–≥–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π —Å–∞–º –≤—ã–∑—ã–≤–∞–µ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç retrieve_context.

    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
        result = generate_agent_rag_reply("–ö–∞–∫ –≥–æ—Ç–æ–≤–∏—Ç—å –ø–ª–æ–≤?")
    """
    try:
        agent = create_rag_agent(model=model)

        base_system_prompt = (
            _build_system_prompt() + "\n\n"
            "–¢—ã –æ—Ç–≤–µ—á–∞–µ—à—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –∏—Å–ø–æ–ª—å–∑—É—è –ª–æ–∫–∞–ª—å–Ω–æ–µ RAG-—Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\n"
            "–£ —Ç–µ–±—è –µ—Å—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç retrieve_context, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\n"
            "–ö–æ–≥–¥–∞ –≤–æ–ø—Ä–æ—Å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤—ã–∑—ã–≤–∞–π —ç—Ç–æ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç. "
            "–ü–æ–¥–∞–≤–∞–π –Ω–∞ –≤—Ö–æ–¥ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ë–ï–ó –∏—Å–∫–∞–∂–µ–Ω–∏–π. "
            "–ó–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–π –ø–æ–ª—É—á–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞. "
            "–ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—Ç —Ç–≤–æ–∏–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º –∑–Ω–∞–Ω–∏—è–º, –¥–æ–≤–µ—Ä—è–π –∫–æ–Ω—Ç–µ–∫—Å—Ç—É."
        )

        if summary:
            base_system_prompt += (
                "\n\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞ –¥–∏–∞–ª–æ–≥–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:\n" + summary
            )

        messages = [
            SystemMessage(content=base_system_prompt),
            HumanMessage(content=question),
        ]

        result = agent.invoke({"messages": messages})

        # –î–ª—è create_agent/CompiledStateGraph —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—ã—á–Ω–æ:
        # {"messages": [SystemMessage, ..., AIMessage]}
        if isinstance(result, dict):
            if "messages" in result:
                msgs = result["messages"]
                if isinstance(msgs, list) and msgs:
                    # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π AIMessage
                    for m in reversed(msgs):
                        if isinstance(m, AIMessage):
                            return (m.content or "").strip()
                    # –ï—Å–ª–∏ AIMessage –Ω–µ –Ω–∞—à–ª–∏ ‚Äî –±–µ—Ä—ë–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ
                    last = msgs[-1]
                    return getattr(last, "content", str(last))
            # –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π ‚Äî –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞ —Å "output"
            if "output" in result:
                return str(result["output"])

        # –§–æ–ª–ª–±–µ–∫ ‚Äî –ø—Ä–æ—Å—Ç–æ —Å—Ç—Ä–æ–∫–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
        return str(result)

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –∞–≥–µ–Ω—Ç–∞: {e}"
