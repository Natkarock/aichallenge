# agent.py ‚Äî live logs (Android LiveData‚Äìstyle) for mid-function updates
import asyncio
import contextvars
import queue as threading_queue
import threading
from typing import Annotated, Sequence, TypedDict, Dict, Any, List
import operator
import os

from dotenv import load_dotenv
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, 
from langgraph.graph.message import add_messages
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.tools import load_mcp_tools
from langchain.agents import create_agent

load_dotenv()

# ---------------- Live logger (context-local), usable from ANY node mid-execution ----------------
_event_queue_ctx: contextvars.ContextVar["asyncio.Queue[dict] | None"] = (
    contextvars.ContextVar("event_q", default=None)
)


async def alog(text: str):
    """Async-safe log: await inside your async node functions to push a line immediately to UI."""
    q = _event_queue_ctx.get()
    if q is not None:
        await q.put({"type": "log", "text": text})


def log(text: str):
    """Sync-safe log: can be used even in sync parts; schedules an async put into the current loop."""
    q = _event_queue_ctx.get()
    if q is not None:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            loop.call_soon_threadsafe(
                asyncio.create_task, q.put({"type": "log", "text": text})
            )


# ---------------- Agent state with logs accumulator (for end-of-node summaries) ------------------
class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    logs: Annotated[List[str], operator.add]


def emit(text: str) -> Dict[str, Any]:
    """Adds a line to the state's log list (captured on node end)."""
    return {"logs": [text]}


# ---------------- MCP tools loading (robust one-by-one) ----------------
REPLICATE_API_TOKEN = os.environ.get("REPLICATE_API_TOKEN", "")


llm_with_tools = ChatOpenAI(
        model="openai:gpt-4.1-mini",
    )


async def get_all_tools():
    servers = {
        "weather_mcp": {
            "transport": "streamable_http",
            "url": "https://weathermcp-natkarock.amvera.io/mcp",
        },
        "image-gen": {
            "transport": "stdio",
            "command": "npx",
            "args": ["@gongrzhe/image-gen-server"],
            "env": {
                "REPLICATE_API_TOKEN": REPLICATE_API_TOKEN,
                "MODEL": os.environ.get("REPLICATE_MODEL"),
            },
        },
        "markdown2pdf": {
            "transport": "stdio",
            # –ò—Å–ø–æ–ª—å–∑—É–π –æ–¥–∏–Ω –∏–∑ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∑–∞–ø—É—Å–∫–∞ ‚Äî –Ω–∞–ø—Ä–∏–º–µ—Ä node build/index.js:
            "command": "node",
            "args": ["/Users/user/MEProjects/markdown2pdf-mcp/build/index.js"],
            "env": {
                "M2P_OUTPUT_DIR": "/Users/user/MEProjects/aichallenge/agent/day_12/weather_image_mcp_agent_v3-2",
                "M2P_VERBOSE": "true",
                # –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏:
                # "M2P_CHROME_PATH": "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome",
                # "M2P_LOAD_TIMEOUT_MS": "120000",
                # "M2P_RENDER_TIMEOUT_MS": "20000",
            },
        },
    }
    client = MultiServerMCPClient(servers)
    tools: List = []
    for name in servers.keys():
        try:
            async with client.session(name) as session:
                t = await load_mcp_tools(session)
                tools.extend(t)
        except Exception as e:
            print(f"[MCP] Failed to load tools from '{name}': {e}")
            # –Ω–µ —Ä–æ–Ω—è–µ–º –∑–∞–≥—Ä—É–∑–∫—É –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
            continue
    return tools


# ---------------- Wrapper that adds a log line without touching node logic ----------------
def with_log(fn, message: str):
    async def _wrapped(state: AgentState) -> AgentState:
        # allow user code to stream mid-function logs:
        # they can call: await alog("...progress...") or log("...progress...")
        out = await fn(state)
        if not isinstance(out, dict):
            out = {}
        # also add a summarized line at node end
        out.update(emit(message))
        return out

    return _wrapped


async def call_weather(state: AgentState) -> AgentState:
    system_prompt = SystemMessage(
        content=(
            "–¢—ã –º–æ—è —Å–∏—Å—Ç–µ–º–∞. –£ —Ç–µ–±—è –µ—Å—Ç—å MCP-–ø–æ–≥–æ–¥–∞"
            "–ê–ª–≥–æ—Ä–∏—Ç–º: (1) –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤—ã–∑–æ–≤–∏ –ø–æ–≥–æ–¥—É (2) –í—ã–≤–µ–¥–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–≥–æ–¥–µ –≤ —á–∏—Ç–∞–µ–º–æ–º –≤–∏–¥–µ"
        )
    )
    messages = [system_prompt] + list(state["messages"])
    # create_agent –æ–∂–∏–¥–∞–µ—Ç {"messages": [...]}
    agent_input = {"messages": messages}
    print("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–≥–æ–¥—ã")
    agent_output = await llm_with_tools.ainvoke(agent_input)
    # agent_output —É–∂–µ dict {"messages":[...]} ‚Äî –≤–µ—Ä–Ω—ë–º –µ–≥–æ –Ω–∞–ø—Ä—è–º—É—é
    return agent_output


async def call_image_generation(state: AgentState) -> AgentState:
    system_prompt = SystemMessage(
        content=(
            "–¢—ã –º–æ—è —Å–∏—Å—Ç–µ–º–∞. –£ —Ç–µ–±—è –µ—Å—Ç—å MCP-–≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π"
            "–ù–∞ –≤—Ö–æ–¥ –ø–æ–ª—É—á–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –Ω–∞–∑–≤–∞–Ω–∏–∏ –≥–æ—Ä–æ–¥–∞ –∏ –ø–æ–≥–æ–¥–µ –≤ –Ω–µ–º"
            "–ê–ª–≥–æ—Ä–∏—Ç–º: (1) –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ –ø–æ–≥–æ–¥–µ. –ü—Ä–æ–º—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–¥–µ–ª–∞–π –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ. (2) –í—ã–≤–µ–¥–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–≥–æ–¥–µ –∏ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö —Å—Å—ã–ª–∫–∞—Ö –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ –ø–æ–≥–æ–¥–µ"
        )
    )
    messages = [system_prompt] + list(state["messages"])
    # create_agent –æ–∂–∏–¥–∞–µ—Ç {"messages": [...]}
    agent_input = {"messages": messages}
    print("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    agent_output = await llm_with_tools.ainvoke(agent_input)
    # agent_output —É–∂–µ dict {"messages":[...]} ‚Äî –≤–µ—Ä–Ω—ë–º –µ–≥–æ –Ω–∞–ø—Ä—è–º—É—é
    return agent_output


async def call_pdf_to_markdown(state: AgentState) -> AgentState:
    system_prompt = SystemMessage(
        content=(
            "–¢—ã –º–æ—è —Å–∏—Å—Ç–µ–º–∞. –£ —Ç–µ–±—è –µ—Å—Ç—å MCP-–≥–µ–Ω–µ—Ä–∞—Ü–∏—è Pdf –∏–∑ markdown"
            "–ù–∞ –≤—Ö–æ–¥ –ø–æ–ª—É—á–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –Ω–∞–∑–≤–∞–Ω–∏–∏ –≥–æ—Ä–æ–¥–∞ –∏ –ø–æ–≥–æ–¥–µ –≤ –Ω–µ–º. –¢–∞–∫–∂–µ –ø–æ–ª—É—á–µ–Ω —Å–ø–∏—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"
            "–ê–ª–≥–æ—Ä–∏—Ç–º: (1) –°—Ñ–æ—Ä–º–∏—Ä—É–π –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≥–æ—Ä–æ–¥–µ –∏–ª–∏ —Å—Ç—Ä–∞–Ω–µ —Å –¥–æ—Å—Ç–æ–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown"
            "(2)–∏–∑ markdown —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π pdf –∏ —Å–æ—Ö—Ä–∞–Ω–∏ –≤ —Ñ–∞–π–ª —á–µ—Ä–µ–∑ mcp"
            "(5)–∑–∞—Ç–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤–µ—Ä–Ω–∏ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ–±—ä–µ–∫—Ç WeatherInfo –ò –ë–û–õ–¨–®–ï –ù–ò–ß–ï–ì–û –ò –ë–û–õ–¨–®–ï –ù–ï –í–´–ó–´–í–ê–ô –ò–ù–°–¢–†–£–ú–ï–ù–¢–´."
            "–í WeatherInfo(json) –ø–æ–º–µ—Å—Ç–∏ –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ–≥–æ–¥—ã , —Å–ø–∏—Å–æ–∫ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ø—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É (description, images, path). –§–∏–Ω–∞–ª—å–Ω—ã–π description –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞ —è–∑—ã–∫–µ –≤–≤–µ–¥–µ–Ω–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è"
        )
    )
    messages = [system_prompt] + list(state["messages"])
    # create_agent –æ–∂–∏–¥–∞–µ—Ç {"messages": [...]}
    agent_input = {"messages": messages}
    print("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è pdf —Ñ–∞–π–ª–æ–≤")
    agent_output = await llm_with_tools.ainvoke(agent_input)
    # agent_output —É–∂–µ dict {"messages":[...]} ‚Äî –≤–µ—Ä–Ω—ë–º –µ–≥–æ –Ω–∞–ø—Ä—è–º—É—é
    return agent_output


async def call_final_answer(state: AgentState) -> AgentState:
    system_prompt = SystemMessage(
        content=(
            "–¢—ã –º–æ—è —Å–∏—Å—Ç–µ–º–∞. –£ —Ç–µ–±—è –µ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–≥–æ–¥–µ, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –ø—É—Ç—å –∫ pdf —Ñ–∞–π–ª—É"
            "–ù–∞ –≤—Ö–æ–¥ –ø–æ–ª—É—á–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –Ω–∞–∑–≤–∞–Ω–∏–∏ –≥–æ—Ä–æ–¥–∞ –∏ –ø–æ–≥–æ–¥–µ –≤ –Ω–µ–º. –¢–∞–∫–∂–µ –ø–æ–ª—É—á–µ–Ω —Å–ø–∏—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –ø—É—Ç—å –∫ pdf —Ñ–∞–π–ª—É"
            "(1)–í–µ—Ä–Ω–∏ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ–±—ä–µ–∫—Ç WeatherInfo –ò –ë–û–õ–¨–®–ï –ù–ò–ß–ï–ì–û –ò –ë–û–õ–¨–®–ï –ù–ï –í–´–ó–´–í–ê–ô –ò–ù–°–¢–†–£–ú–ï–ù–¢–´."
            "–í WeatherInfo(json) –ø–æ–º–µ—Å—Ç–∏ –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ–≥–æ–¥—ã , —Å–ø–∏—Å–æ–∫ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ø—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É (description, images, path). –§–∏–Ω–∞–ª—å–Ω—ã–π description –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞ —è–∑—ã–∫–µ –≤–≤–µ–¥–µ–Ω–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è"
        )
    )
    messages = [system_prompt] + list(state["messages"])
    # create_agent –æ–∂–∏–¥–∞–µ—Ç {"messages": [...]}
    agent_input = {"messages": messages}
    print("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞")
    agent_output = await llm_with_tools.ainvoke(agent_input)
    # agent_output —É–∂–µ dict {"messages":[...]} ‚Äî –≤–µ—Ä–Ω—ë–º –µ–≥–æ –Ω–∞–ø—Ä—è–º—É—é
    return agent_output


# ---------------- Build graph using your existing node functions ----------------
def _build_graph(all_tools):
    # IMPORTANT: we do NOT change your business logic; we assume these functions exist in this file:
    # - call_weather(state) -> AgentState
    # - call_image_generation(state) -> AgentState
    # - call_pdf_to_markdown(state) -> AgentState
    # - call_final_answer(state) -> AgentState
    # If they are in another module, import them above and keep the same names.

    # Create ToolNode (don't fail graph on tool errors)
    tool_node = ToolNode(tools=all_tools, handle_tool_errors=True)

    graph = StateGraph(AgentState)

    # Wrap your nodes with with_log(...) so we append a line when a node ends,
    # and you can stream mid-function via alog()/log().
    graph.add_node("call_weather", with_log(call_weather, "üå§Ô∏è –®–∞–≥ 1/4: –ø–æ–≥–æ–¥–∞ –ø–æ–ª—É—á–µ–Ω–∞"))
    graph.add_node(
        "call_image_generation",
        with_log(call_image_generation, "üñºÔ∏è –®–∞–≥ 2/4: –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≥–æ—Ç–æ–≤–æ"),
    )
    graph.add_node(
        "call_pdf_to_markdown",
        with_log(call_pdf_to_markdown, "üìÑ –®–∞–≥ 3/4: PDF —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω"),
    )
    graph.add_node(
        "call_final_answer",
        with_log(call_final_answer, "‚úÖ –®–∞–≥ 4/4: —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω"),
    )

    graph.add_node("tools", tool_node)

    # Keep your edges/order:
    graph.add_edge(START, "call_weather")
    graph.add_edge("call_weather", "tools")
    graph.add_edge("tools", "call_image_generation")
    graph.add_edge("call_image_generation", "tools")
    graph.add_edge("tools", "call_pdf_to_markdown")
    graph.add_edge("call_pdf_to_markdown", "tools")
    graph.add_edge("tools", "call_final_answer")
    graph.add_edge("call_final_answer", END)

    return graph.compile()


# ---------------- Legacy API: return final result (no streaming) ----------------
async def run_with_message(message: str) -> Dict[str, Any]:
    all_tools = await get_all_tools()
    app = _build_graph(all_tools)
    result = await app.ainvoke(
        {"messages": [HumanMessage(content=message)], "logs": []}
    )
    return result


# ---------------- Live streaming API: yield events during execution ----------------
def run_with_message_events(message: str):
    """Sync generator for Streamlit: emits {'type':'log','text':...} and then {'type':'final','result':...}.

    Use alog()/log() in your nodes to stream mid-function progress to UI immediately.
    """
    thread_q: "threading_queue.Queue[dict | None]" = threading_queue.Queue(maxsize=500)

    async def producer():
        # internal async queue visible to nodes via context var
        async_q: "asyncio.Queue[dict]" = asyncio.Queue(maxsize=1000)
        token = _event_queue_ctx.set(async_q)
        try:
            all_tools = await get_all_tools()
            app = _build_graph(all_tools)

            # run the graph in background
            task = asyncio.create_task(
                app.ainvoke({"messages": [HumanMessage(content=message)], "logs": []})
            )

            # forward any logs coming from nodes immediately
            async def forwarder():
                while True:
                    item = await async_q.get()
                    thread_q.put(item)  # send to Streamlit thread
                    async_q.task_done()

            fwd = asyncio.create_task(forwarder())

            # also emit node start/end via astream_events for nice UX
            async for ev in app.astream_events(
                {"messages": [HumanMessage(content=message)], "logs": []}, version="v1"
            ):
                if ev.get("event") == "on_node_start":
                    node = ev.get("name", "")
                    thread_q.put({"type": "log", "text": f"‚ñ∂Ô∏è {node}‚Ä¶"})
                if ev.get("event") == "on_node_end" and ev.get("name") == "graph":
                    # graph finished; result is in ev['data']['output'] too, but we also wait for task
                    pass

            result = await task
            fwd.cancel()
            thread_q.put({"type": "final", "result": result})
        finally:
            _event_queue_ctx.reset(token)
            thread_q.put(None)

    def runner():
        asyncio.run(producer())

    threading.Thread(target=runner, daemon=True).start()

    while True:
        item = thread_q.get()
        if item is None:
            break
        yield item


# --------------- NOTE for your node functions ---------------
# Inside your existing nodes you can now do:
#
#   from agent import alog, log
#   await alog("–ó–∞–ø—Ä–∞—à–∏–≤–∞—é –≥–µ–æ–∫–æ–¥–∏–Ω–≥‚Ä¶")       # mid-function, async
#   log("–ü–æ–ª—É—á–µ–Ω—ã –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã: 45.0, 38.9")    # mid-function, sync-safe
#
# These lines will appear in Streamlit immediately, before the node finishes.
